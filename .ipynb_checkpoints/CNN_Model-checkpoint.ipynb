{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten,BatchNormalization,Activation,Conv2D\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras import optimizers, regularizers, Model\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join \n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter\n",
    "from glob import iglob\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "\n",
    "# Import OpenCV\n",
    "import cv2\n",
    " \n",
    "# Ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (256,256)\n",
    "INPUT_SHAPE = (256,256,3)\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "EPOCHS = 20\n",
    "STEP_PER_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'dataset_v2'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "predict_dir = os.path.join(base_dir, 'predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating objects for image augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1890 images belonging to 5 classes.\n",
      "Found 418 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   vertical_flip=True,\n",
    "                                   rotation_range=90)\n",
    " \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                            target_size=IMAGE_SIZE,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            class_mode='categorical')\n",
    " \n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        directory=test_dir,\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "model.add(Conv2D(32, (3, 3), input_shape = INPUT_SHAPE, activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "# model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding a third convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding a fourth convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "model.add(Flatten())   \n",
    "# Step 4 - Full connection\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units = 1024, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(units = 5, activation = 'softmax'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD(lr=0.0001, momentum=0.9, decay=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 196s 1s/step - loss: 1.4592 - accuracy: 0.4936 - val_loss: 1.9014 - val_accuracy: 0.2745\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 178s 1s/step - loss: 1.2851 - accuracy: 0.5538 - val_loss: 2.2352 - val_accuracy: 0.2475\n",
      "Epoch 3/20\n",
      " 17/157 [==>...........................] - ETA: 2:35 - loss: 1.1424 - accuracy: 0.5931"
     ]
    }
   ],
   "source": [
    "STEP_PER_EPOCHS = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = test_generator.n//test_generator.batch_size\n",
    "\n",
    "model_history = model.fit(train_generator,steps_per_epoch= STEP_PER_EPOCHS,\n",
    "                    epochs=EPOCHS, validation_data=test_generator, validation_steps=STEP_SIZE_VALID,\n",
    "                    callbacks=[EarlyStopping(monitor='loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy and Loss Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "def model_performance_plot(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "  \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['accuracy'],label='Training Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['val_accuracy'],label = 'Validation Accuracy')\n",
    "    plt.legend()\n",
    "  \n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Training and Aalidation Categorical Crossentropy Loss')\n",
    "    plt.plot(hist['epoch'], hist['loss'],label='Training Loss')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'],label = 'Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model_performance_plot(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate_generator(test_generator, steps=test_generator.samples//test_generator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test dataset Accuracy: %f and Loss: %f\" % (accuracy,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Classification and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_generator(test_generator, len(test_generator))\n",
    "y_pred = np.argmax(Y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = list(test_generator.class_indices.keys())\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(test_generator.classes, y_pred)\n",
    "df_cm = pd.DataFrame(conf_mat, index = target_names, columns = target_names)\n",
    "plt.figure(figsize = (12,8))\n",
    "sn.heatmap(df_cm, annot=True,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {v:k for k,v in train_generator.class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_type(filename):\n",
    "    \n",
    "    if \"glass\" in filename:\n",
    "        return 'glass'\n",
    "    elif \"metal\" in filename:\n",
    "        return 'metal'\n",
    "    elif \"paper\" in filename:\n",
    "        return 'paper'\n",
    "    elif \"plastic\" in filename:\n",
    "        return 'plastic'\n",
    "    elif \"cardboard\" in filename:\n",
    "        return 'cardboard'\n",
    "    \n",
    "\n",
    "def plot_predicted_label(nrows, ncols,onlyfiles):\n",
    "    fig, axes = plt.subplots(nrows, ncols,figsize=(20, 13))\n",
    "    \n",
    "    rs = np.random.choice(len(onlyfiles),nrows*ncols,replace=False)\n",
    "    \n",
    "    for i, ax in zip(rs, axes.flat):\n",
    "        img = cv2.imread(os.path.join(predict_dir, onlyfiles[i]))\n",
    "        true_type = image_type(onlyfiles[i])\n",
    "        img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1])) \n",
    "        img = img /255\n",
    "         \n",
    "        probabilities = model.predict(np.asarray([img]))[0]\n",
    "        class_idx = np.argmax(probabilities)\n",
    "            \n",
    "        title = 'True: %s, Pred: %s , Confi:%0.2f' % (true_type,class_mapping[class_idx],probabilities[class_idx])\n",
    "        ax.imshow(img, cmap='binary')\n",
    "        ax.set_title(title)\n",
    "         \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        \n",
    "num_rows = 2\n",
    "num_cols = 4\n",
    "\n",
    "# load image path to predict\n",
    "onlyfiles = [f for f in listdir(predict_dir) if isfile(join(predict_dir, f))]\n",
    "\n",
    "plot_predicted_label(num_rows, num_cols,onlyfiles);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"CNNModel.pkl\"\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "    \n",
    "\n",
    "model.save('CNNModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
